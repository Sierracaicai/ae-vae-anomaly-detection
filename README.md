# ðŸ” AE-VAE Anomaly Detection

This project evaluates and compares anomaly detection approaches on network traffic data using:

- ðŸ§  Deep learning methods: Autoencoder (AE), Variational Autoencoder (VAE)
- ðŸ§ª Classical methods: Isolation Forest, One-Class SVM, Local Outlier Factor, Elliptic Envelope

> Built as a portfolio-grade, modular and reproducible data science project.

---

## ðŸ“ Project Structure

```
ae-vae-anomaly-detection/
â”œâ”€â”€ images/                 # save all plots
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md           # Dataset Description
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ feature_review.md  
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚     â”œâ”€â”€ ae_eval_report.md      
â”‚   â”‚     â”œâ”€â”€ ae_depth_comparison_report.md
â”‚   â”‚     â”œâ”€â”€ bottleneck_report.md
â”‚   â”‚     â”œâ”€â”€ ae_activation_experiment.md
â”‚   â”‚     â”œâ”€â”€ ae_mixed_loss_experiment.md
â”‚   â”‚     â”œâ”€â”€ optimizer_experiment.md
â”‚   â”‚     â””â”€â”€ thresholding_comparison.md
â”‚   â”œâ”€â”€ final_ae_experiment_report.md  
â”‚   â”œâ”€â”€ VAE_report.md       
â”‚   â”œâ”€â”€ AE_vs_Traditional_Report.md 
â”‚   â””â”€â”€ SHAP_Interpretability_Report.md 
â”œâ”€â”€ notebooks/              # Jupyter notebooks by stage
â”‚   â”œâ”€â”€ 0_draft_experiments.ipynb 
â”‚   â”œâ”€â”€ 0_exploration_and_baseline.ipynb 
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Preprocess_and_TSNE.ipynb
â”‚   â”œâ”€â”€ AE_base_model.ipynb
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚    â”œâ”€â”€ AE_Depth_Comparison.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_Bottleneck_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_activation_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ ae_mixed_loss_experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ AE_optimizer_experiment.ipynb
â”‚   â”‚    â””â”€â”€ AE_Adaptive_Thresholding_experiment.ipynb
â”‚   â”œâ”€â”€ VAE_model.ipynb
â”‚   â”œâ”€â”€ traditional_models_with_tuning.ipy
â”‚   â””â”€â”€ Shap_Interpretability.ipynb
â”‚   
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/              # Reusable utility modules
â”‚   â”‚   â”œâ”€â”€ load_data.py    # Load raw CSV with default column names
â”‚   â”‚   â”œâ”€â”€ load_data.md
â”‚   â”‚   â”œâ”€â”€ reduce_mem.py   # Downcast dtypes to reduce memory
â”‚   â”‚   â”œâ”€â”€ reduce_mem.md
â”‚   â”‚   â”œâ”€â”€ preprocess.py   # Full preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ preprocess.md
â”‚   â”‚   â”œâ”€â”€ eda_tools.py    # EDA plotting & statistics utilities
â”‚   â”‚   â”œâ”€â”€ eda_tools.md
â”‚   â”‚   â”œâ”€â”€ tsne_vis.py     # t-SNE 2D projection tool
â”‚   â”‚   â”œâ”€â”€ tsne_vis.md
â”‚   â”‚   â””â”€â”€ module_reload.py# Hot-reload Python modules in Colab
â”‚   â””â”€â”€models/
â”‚       â”œâ”€â”€ ae_model.py           
â”‚       â”œâ”€â”€ ae_model.md 
â”‚       â”œâ”€â”€ ae_evaluation.py   
â”‚       â”œâ”€â”€ ae_evaluation.md  
â”‚       â”œâ”€â”€ thresholding.py
â”‚       â”œâ”€â”€ vae_model.py
â”‚       â”œâ”€â”€ beta_vae_model.py
â”‚       â””â”€â”€ best_ae.h5    
â”‚
â”œâ”€â”€ requirements.txt        # List required Python packages & version
â”œâ”€â”€ .gitignore              # Exclude raw/processed data, env files
â””â”€â”€ README.md               # Project overview & usage
```

---

## ðŸ“Š Dataset

**UNSW-NB15** â€” A modern labeled network intrusion detection dataset.

- 47 features + `attack_cat` + binary `label`
- Mixture of normal traffic and multiple attack types
- Sampling Strategy
  - Initial experiments: 1/10 data for fast testing
  - Final AE/VAE training: full cleaned dataset (~640,000 rows)
  - All models trained on normal samples only, tested on mixed samples
- See `data/README.md` for detailed download and preparation instructions
---


## ðŸ§° Core Modules

This project is organized into clear functional components to enhance reproducibility and modularity:

### ðŸ”§ `src/utils/`
Utility scripts for preprocessing, EDA, dimensionality reduction:
- `preprocess.py`: data cleaning pipeline, encoding, scaling, feature reduction
- `load_data.py`: standardized CSV loading with optional dtype control
- `reduce_mem.py`: memory optimization for large CSVs
- `tsne_vis.py`: TSNE dimensionality reduction for visualization
- `eda_tools.py`: helper functions for EDA and summary stats

### ðŸ¤– `src/models/`
Core model definitions and training utilities:
- `ae_model.py`: AutoEncoder model builder (depth, activation, dropout adjustable)
- `vae_model.py`: VAE model builder 
- `beta_vae_model.py`: Î²-VAE implementation with KL tuning and warm-up
- `ae_evaluation.py`: evaluation utilities, visualizations, and reporting
- `thresholding.py`: percentile/F1 threshold strategies

Each model module is independently testable and documented inline.



## ðŸ““ Notebooks

All notebooks are stored in the [`notebooks/`](./notebooks) folder and organized for each stage:

| Notebook | Description |
|----------|-------------|
| `0_draft_experiments.ipynb` | The very first draft version of this project |
| `0_exploration_and_baseline.ipynb` | A more structured and organized version of the original baseline |
| `ðŸ”ºEDA.ipynb` | Exploratory data analysis, feature distribution, rare category check |
| `ðŸ”ºPreprocess_and_TSNE.ipynb` | Preprocessing pipeline, scaling, encoding, and latent space visualization |
| `ðŸ”ºAE_base_model.ipynb` | Shallow AE structure, training, loss curves, evaluation |
| `ðŸ”ºexperiments/` | 6 controlled AE model experiments on depth, bottleneck size, activations, loss functions, optimizers, thresholds |
Each experiment is separately documented and reproducible.
| `ðŸ”ºVAE_model.ipynb` | VAE and Î²-VAE models with KL warm-up and visualization |
| `ðŸ”ºShap_Interpretability.ipynb` | SHAP analysis for best AE model interpretability |
| `ðŸ”ºtraditional_models_with_tuning.ipynb` | Baseline classical models: One-Class SVM, LOF, Isolation Forest, Elliptic Envelope and evaluation|
---

## ðŸ”¬ Models and Experiments

The goal is to design and evaluate anomaly detection models under a robust framework. Models evaluated include:

### âœ… AutoEncoder (AE)

Try basic AE model first, then design 6 experiments below to refine it
- Shallow & Deep AE comparison
- Bottleneck dimension sweep (4, 8, 16, 32)
- Activation comparison: ReLU, Tanh, ELU, SELU, LeakyReLU
- Loss strategies: standard MSE vs. mixed loss
- Optimizers: Adam, AdamW, SGD
- Thresholding: Percentile-based, F1-maximization

ðŸ“Š **Best AE Result**  
F1 = 0.7527 | AUC = 0.9907 (Shallow AE, tanh, bottleneck=16, AdamW)

### âœ… Variational AutoEncoder (VAE)

- Basic VAE
- KL divergence weighting (`Î²`-VAE)
- KL annealing (warm-up strategy)

âŒ Paused further optimization on the VAE series because they underperformed compared to AE on this dataset

### ðŸ†š Traditional Models

- One-Class SVM, Isolation Forest, Local Outlier Factor, Elliptic Envelope
- Trained on **normal-only samples**, validated on mixed data
- AE consistently outperformed all classical methods in F1/AUC

---
## ðŸ” SHAP Interpretability

To better understand the behavior of the trained AutoEncoder (AE) model, I employed **SHAP (SHapley Additive exPlanations)** to provide insights into which input features most influence anomaly scores.

### â“ Why SHAP?

SHAP is a unified approach to explain the output of any machine learning model. It computes the contribution of each feature by considering all possible combinations of features. In the context of anomaly detection, this allows us to:

- Identify which features contribute most to reconstruction errors
- Investigate what drives anomalous vs. normal behavior
- Build trust and transparency into the modelâ€™s decisions

### ðŸ“ˆ Summary Plot

The SHAP summary plot ranks features by average impact and shows how high/low values of each feature affect anomaly detection. Here is the summary plot for our final selected AE model:![](images/SHAP_sum_plot.png)


### Workflow

- A subset of **anomalous** samples was selected for explanation
- SHAP values were computed using a background sample from normal data
- We visualized global importance using summary and bar plots

SHAP results demonstrated the AE model's ability to focus on semantically meaningful indicators of attack behavior.

> ðŸ“ For detailed SHAP plots and explanation workflow, see `notebooks/Shap_Interpretability.ipynb`.

## ðŸ Final Results: AE vs Traditional Methods

After a series of architectural, training, and loss function optimizations, the final AutoEncoder (AE) model was selected based on its balanced performance across precision, recall, and F1 score.

### ðŸ“Š Final AE Performance
| Metric        | Value     |
|---------------|-----------|
| Precision     | 0.60896   |
| Recall        | 0.98529   |
| F1 Score      | 0.75271   |
| ROC AUC       | 0.99079   |
| TP / FP / TN / FN | 2814 / 1807 / 123495 / 42 |

### ðŸ§ª Traditional Methods Performance Comparison

| Method              | Precision | Recall  | F1 Score | ROC AUC | Notes |
|---------------------|-----------|---------|----------|---------|-------|
| **One-Class SVM**   | 0.55390   | 0.54517 | 0.54950  | 0.76758 | Tuned with GridSearch |
| **Isolation Forest**| 0.32598   | 0.21218 | 0.25705  | 0.60109 | Weak performance overall |
| **Elliptic Envelope** | 0.43888 | 0.34069 | 0.38360  | 0.66538 | Assumes Gaussian |
| **Local Outlier Factor** | 0.55393 | 0.54307 | 0.54844 | 0.76655 | Unsupervised fit on normal only |

The AE model **outperforms all traditional models** across all major metrics.

## ðŸ”­ Future Work

This project lays a strong foundation for applying Autoencoder (AE) and Variational Autoencoder (VAE) models to network anomaly detection. Based on current results and limitations, several future directions are planned:

- **Custom Loss Functions**: Explore more advanced loss functions such as perceptual loss or feature-level reconstruction to better capture anomaly structure.
- **Model Fusion**: Investigate the combination of AE/VAE outputs with traditional anomaly detection scores (e.g., One-Class SVM) using ensemble techniques.
- **Generalization to Other Datasets**: Validate the current model pipeline on diverse cybersecurity datasets (e.g., CICIDS2017, NSL-KDD).
- **Online or Continual Learning**: Extend the AE/VAE to adapt in real-time or on streaming network traffic.
- **Time-Series Modeling**: Incorporate sequential architectures like LSTM-AE or Transformer-based VAE for traffic behavior modeling.
- **End-to-End Deployment**: Design a lightweight model variant suitable for deployment in real-time intrusion detection systems (IDS).

These directions aim to improve performance, robustness, interpretability, and deployment-readiness of deep anomaly detection models in real-world security settings.

---

## ðŸ‘©â€ðŸ’» Author

Created by [Shiyu Cai] Â· Portfolio Project  
> *Feel free to fork, study or reuse parts of this repo in your own data science learning path.*