# ðŸ” AE-VAE Anomaly Detection

This project evaluates and compares anomaly detection approaches on network traffic data using:

- ðŸ§  Deep learning methods: Autoencoder (AE), Variational Autoencoder (VAE)
- ðŸ§ª Classical methods: Isolation Forest, One-Class SVM, Local Outlier Factor, Elliptic Envelope

> Built as a portfolio-grade, modular and reproducible data science project.

---

## ðŸ“ Project Structure

```
ae-vae-anomaly-detection/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md           # Dataset Description
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ feature_review.md  
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚     â”œâ”€â”€ ae_eval_report.md      
â”‚   â”‚     â”œâ”€â”€ ae_depth_comparison_report.md
â”‚   â”‚     â”œâ”€â”€ bottleneck_report.md
â”‚   â”‚     â”œâ”€â”€ ae_activation_experiment.md
â”‚   â”‚     â”œâ”€â”€ ae_mixed_loss_experiment.md
â”‚   â”‚     â”œâ”€â”€ optimizer_experiment.md
â”‚   â”‚     â””â”€â”€ thresholding_comparison.md
â”‚   â”œâ”€â”€ final_ae_experiment_report.md  
â”‚   â”œâ”€â”€ VAE_report.md       
â”‚   â”œâ”€â”€ AE_vs_Traditional_Report.md 
â”‚   â””â”€â”€ SHAP_Interpretability_Report.md 
â”œâ”€â”€ notebooks/              # Jupyter notebooks by stage
â”‚   â”œâ”€â”€ 0_draft_experiments.ipynb 
â”‚   â”œâ”€â”€ 0_exploration_and_baseline.ipynb 
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Preprocess_and_TSNE.ipynb
â”‚   â”œâ”€â”€ AE_base_model.ipynb
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚    â”œâ”€â”€ AE_Depth_Comparison.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_Bottleneck_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_activation_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ ae_mixed_loss_experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ AE_optimizer_experiment.ipynb
â”‚   â”‚    â””â”€â”€ AE_Adaptive_Thresholding_experiment.ipynb
â”‚   â”œâ”€â”€ VAE_model.ipynb
â”‚   â”œâ”€â”€ traditional_models_with_tuning.ipy
â”‚   â”œâ”€â”€ Shap_Interpretability.ipynb
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/              # Reusable utility modules
â”‚   â”‚   â”œâ”€â”€ load_data.py    # Load raw CSV with default column names
â”‚   â”‚   â”œâ”€â”€ load_data.md
â”‚   â”‚   â”œâ”€â”€ reduce_mem.py   # Downcast dtypes to reduce memory
â”‚   â”‚   â”œâ”€â”€ reduce_mem.md
â”‚   â”‚   â”œâ”€â”€ preprocess.py   # Full preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ preprocess.md
â”‚   â”‚   â”œâ”€â”€ eda_tools.py    # EDA plotting & statistics utilities
â”‚   â”‚   â”œâ”€â”€ eda_tools.md
â”‚   â”‚   â”œâ”€â”€ tsne_vis.py     # t-SNE 2D projection tool
â”‚   â”‚   â”œâ”€â”€ tsne_vis.md
â”‚   â”‚   â””â”€â”€ module_reload.py# Hot-reload Python modules in Colab
â”‚   â””â”€â”€models/
â”‚       â”œâ”€â”€ ae_model.py           
â”‚       â”œâ”€â”€ ae_model.md 
â”‚       â”œâ”€â”€ ae_evaluation.py   
â”‚       â”œâ”€â”€ ae_evaluation.md  
â”‚       â”œâ”€â”€ thresholding.py
â”‚       â”œâ”€â”€ vae_model.py
â”‚       â”œâ”€â”€ beta_vae_model.py
â”‚       â””â”€â”€ best_ae.h5    
â”‚
â”œâ”€â”€ requirements.txt        # List required Python packages & version
â”œâ”€â”€ .gitignore              # Exclude raw/processed data, env files
â””â”€â”€ README.md               # Project overview & usage
```

---

## ðŸ“Š Dataset

**UNSW-NB15** â€” A modern labeled network intrusion detection dataset.

- 49 features + `attack_cat` + binary `label`
- Mixture of normal traffic and multiple attack types
- Used only the first 100,000 rows for initial development (scalable)

---

## ðŸ§° Core Modules

### ðŸ”¹ EDA Tools (`src/utils/eda_tools.py`)

- Distribution histograms, label counts
- Outlier detection (Z-score)
- Correlation heatmaps
- Distribution drift analysis (KS-test)
- Rare category detection

ðŸ“˜ See [docs/EDA Tools](src/utils/eda_tools.md)

### ðŸ”¹ Data Preprocessing (`src/utils/preprocess.py`)

- Categorical encoding (one-hot)
- MinMax/Standard scaling
- Optional log1p transformation
- Auto correlation filtering
- Cleaned output saved to CSV

ðŸ“˜ See [docs/Data Preprocessing](src/utils/preprocess.md)

### ðŸ”¹ Data Loader (`src/utils/load_data.py`)

- Loads raw CSV with default UNSW column names
- Used to decouple EDA from preprocessing

ðŸ“˜ See [docs/Data Loader](src/utils/load_data.md)

### ðŸ”¹ Memory Optimizer (`src/utils/reduce_mem.py`)

- Downcasts numerical columns to reduce memory usage
- Especially useful in Colab or large dataset scenarios
- Optional float16 support
- Automatically prints memory reduction summary

ðŸ“˜ See [docs/Memory Optimizer](src/utils/reduce_mem.md)


### ðŸ”¹ Colab Module Hot Reload (`src/utils/module_reload.py`)

- âš¡ Reloads all utility `.py` modules (preprocess, EDA, t-SNE, etc.) **without restarting the Colab runtime**
- Designed for **modular workflows** in Google Colab
- Automatically reloads:
  - `preprocess.py`
  - `eda_tools.py`
  - `tsne_vis.py`
  - `load_data.py`
  - `reduce_mem.py`

ðŸ§ª Run in Colab anytime after modifying a `.py`:

```python
%run /content/src/utils/module_reload.py
```

### ðŸ”¹ t-SNE Projection Tool (`src/utils/tsne_vis.py`)

- Visualizes high-dimensional data (e.g., features after preprocessing or latent encodings) in 2D space
- Helps assess the separability of normal vs anomaly samples
- Automatically samples from both classes and supports standardization toggle

ðŸ“˜ See [docs/t-SNE Projection Tool](src/utils/tsne_vis.md)

## ðŸ““ Notebooks

- `0_draft_experiments.ipynb`: The very first draft version  
  - Located in [`notebooks/`](notebooks/)

- `0_exploration_and_baseline.ipynb`: A more structured and organized version of the original baseline
  - Located in [`notebooks/`](notebooks/)

- `EDA.ipynb`: Full exploratory data analysis (EDA) including:
  - Shape, types, nulls, duplicates
  - Label & category distributions
  - Feature histograms, skewness, correlation
  - Optional concept drift check (KS-test, normal-only)
  - Located in [`notebooks/`](notebooks/)

- `Preprocess_and_TSNE.ipynb`: To be completed


---

## ðŸ”¬ Models and Experiments

| Model            | Type         | Status  | Notes                                   |
|------------------|--------------|---------|-----------------------------------------|
| Autoencoder      | Deep Learning | âœ… Done | Shallow AE with Dropout + BN            |
| VAE              | Deep Learning | âœ… Done | Custom training loop + KL-div loss      |
| Isolation Forest | Traditional  | âœ… Done | Baseline comparison                      |
| One-Class SVM    | Traditional  | âœ… Done | Baseline comparison                      |

ðŸ“ˆ Metrics: MSE reconstruction error, AUC, precision, recall

---

## ðŸ§ª Experimental Design

- Trained AE/VAE on **normal samples only**
- Tested on **mixed samples (normal + anomalous)**
- Compared anomaly scores across methods
- Evaluated impact of:
  - Sample size
  - Thresholding method
  - Feature engineering
  - Model complexity

---


## ðŸ”­ Future Work
- Add interpretability via SHAP / LIME
- Hyperparameter tuning and AE/VAE deepening
- Build modular `train_runner.py`
- Add CLI + logging support for reproducible runs
- **Strict â€œTrain-Onlyâ€ Feature Engineering**  
  Implement a fully pipeline-based workflow (e.g. custom `RareCategoryGrouper`, correlation filters, PCA) that is **fitted on the training set only** and then applied to validation/test sets. This will eliminate any potential leakage from unsupervised transforms.

- **Compare Leakage-Aware vs. Pragmatic Approaches**  
  Empirically evaluate the impact on AE/VAE and traditional model performance when grouping rare categories and dropping correlations using (a) full-data rules vs. (b) train-only rules.

- **Production-Ready SKLearn Pipeline**  
  Wrap the entire preprocess â†’ train â†’ evaluate steps into an `sklearn.Pipeline` (or TensorFlow Transform) to support reproducible CI/CD retraining and deployment.

- **Automated CI/CD Tests**  
  Add unit/integration tests that assert no data-leakageâ€”e.g. confirm that transformers fitted on train data yield identical outputs on unseen samples.

- **Monitoring & Drift Detection in Deployment**  
  In a deployed setting, continuously monitor feature distributions and retrain the â€œtrain-onlyâ€ pipeline when concept drift is detected.


---

## ðŸ‘©â€ðŸ’» Author

Created by [Your Name] Â· Portfolio Project  
> *Feel free to fork, study or reuse parts of this repo in your own data science learning path.*