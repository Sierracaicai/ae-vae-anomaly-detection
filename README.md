# ðŸ” AE-VAE Anomaly Detection

This project evaluates and compares anomaly detection approaches on network traffic data using:

- ðŸ§  Deep learning methods: Autoencoder (AE), Variational Autoencoder (VAE)
- ðŸ§ª Classical methods: Isolation Forest, One-Class SVM, Local Outlier Factor, Elliptic Envelope

> Built as a portfolio-grade, modular and reproducible data science project.

---

## ðŸ“ Project Structure

```
ae-vae-anomaly-detection/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md           # Dataset Description
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ feature_review.md  
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚     â”œâ”€â”€ ae_eval_report.md      
â”‚   â”‚     â”œâ”€â”€ ae_depth_comparison_report.md
â”‚   â”‚     â”œâ”€â”€ bottleneck_report.md
â”‚   â”‚     â”œâ”€â”€ ae_activation_experiment.md
â”‚   â”‚     â”œâ”€â”€ ae_mixed_loss_experiment.md
â”‚   â”‚     â”œâ”€â”€ optimizer_experiment.md
â”‚   â”‚     â””â”€â”€ thresholding_comparison.md
â”‚   â”œâ”€â”€ final_ae_experiment_report.md  
â”‚   â”œâ”€â”€ VAE_report.md       
â”‚   â”œâ”€â”€ AE_vs_Traditional_Report.md 
â”‚   â””â”€â”€ SHAP_Interpretability_Report.md 
â”œâ”€â”€ notebooks/              # Jupyter notebooks by stage
â”‚   â”œâ”€â”€ 0_draft_experiments.ipynb 
â”‚   â”œâ”€â”€ 0_exploration_and_baseline.ipynb 
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Preprocess_and_TSNE.ipynb
â”‚   â”œâ”€â”€ AE_base_model.ipynb
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚    â”œâ”€â”€ AE_Depth_Comparison.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_Bottleneck_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ Ae_activation_Experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ ae_mixed_loss_experiment.ipynb
â”‚   â”‚    â”œâ”€â”€ AE_optimizer_experiment.ipynb
â”‚   â”‚    â””â”€â”€ AE_Adaptive_Thresholding_experiment.ipynb
â”‚   â”œâ”€â”€ VAE_model.ipynb
â”‚   â”œâ”€â”€ traditional_models_with_tuning.ipy
â”‚   â””â”€â”€ Shap_Interpretability.ipynb
â”‚   
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/              # Reusable utility modules
â”‚   â”‚   â”œâ”€â”€ load_data.py    # Load raw CSV with default column names
â”‚   â”‚   â”œâ”€â”€ load_data.md
â”‚   â”‚   â”œâ”€â”€ reduce_mem.py   # Downcast dtypes to reduce memory
â”‚   â”‚   â”œâ”€â”€ reduce_mem.md
â”‚   â”‚   â”œâ”€â”€ preprocess.py   # Full preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ preprocess.md
â”‚   â”‚   â”œâ”€â”€ eda_tools.py    # EDA plotting & statistics utilities
â”‚   â”‚   â”œâ”€â”€ eda_tools.md
â”‚   â”‚   â”œâ”€â”€ tsne_vis.py     # t-SNE 2D projection tool
â”‚   â”‚   â”œâ”€â”€ tsne_vis.md
â”‚   â”‚   â””â”€â”€ module_reload.py# Hot-reload Python modules in Colab
â”‚   â””â”€â”€models/
â”‚       â”œâ”€â”€ ae_model.py           
â”‚       â”œâ”€â”€ ae_model.md 
â”‚       â”œâ”€â”€ ae_evaluation.py   
â”‚       â”œâ”€â”€ ae_evaluation.md  
â”‚       â”œâ”€â”€ thresholding.py
â”‚       â”œâ”€â”€ vae_model.py
â”‚       â”œâ”€â”€ beta_vae_model.py
â”‚       â””â”€â”€ best_ae.h5    
â”‚
â”œâ”€â”€ requirements.txt        # List required Python packages & version
â”œâ”€â”€ .gitignore              # Exclude raw/processed data, env files
â””â”€â”€ README.md               # Project overview & usage
```

---

## ðŸ“Š Dataset

**UNSW-NB15** â€” A modern labeled network intrusion detection dataset.

- 47 features + `attack_cat` + binary `label`
- Mixture of normal traffic and multiple attack types
- Sampling Strategy
  - Initial experiments: 1/10 data for fast testing
  - Final AE/VAE training: full cleaned dataset (~640,000 rows)
  - All models trained on normal samples only, tested on mixed samples
- See `data/README.md` for detailed download and preparation instructions
---


## ðŸ§° Core Modules

This project is organized into clear functional components to enhance reproducibility and modularity:

### ðŸ”§ `src/utils/`
Utility scripts for preprocessing, EDA, dimensionality reduction:
- `preprocess.py`: data cleaning pipeline, encoding, scaling, feature reduction
- `load_data.py`: standardized CSV loading with optional dtype control
- `reduce_mem.py`: memory optimization for large CSVs
- `tsne_vis.py`: TSNE dimensionality reduction for visualization
- `eda_tools.py`: helper functions for EDA and summary stats

### ðŸ¤– `src/models/`
Core model definitions and training utilities:
- `ae_model.py`: AutoEncoder model builder (depth, activation, dropout adjustable)
- `vae_model.py`: VAE model builder 
- `beta_vae_model.py`: Î²-VAE implementation with KL tuning and warm-up
- `ae_evaluation.py`: evaluation utilities, visualizations, and reporting
- `thresholding.py`: percentile/F1 threshold strategies

Each model module is independently testable and documented inline.



## ðŸ““ Notebooks

All notebooks are stored in the [`notebooks/`](./notebooks) folder and organized for each stage:

| Notebook | Description |
|----------|-------------|
| `0_draft_experiments.ipynb` | The very first draft version of this project |
| `0_exploration_and_baseline.ipynb` | A more structured and organized version of the original baseline |
| `ðŸ”ºEDA.ipynb` | Exploratory data analysis, feature distribution, rare category check |
| `ðŸ”ºPreprocess_and_TSNE.ipynb` | Preprocessing pipeline, scaling, encoding, and latent space visualization |
| `ðŸ”ºAE_base_model.ipynb` | Shallow AE structure, training, loss curves, evaluation |
| `ðŸ”ºexperiments/` | 6 controlled AE model experiments on depth, bottleneck size, activations, loss functions, optimizers, thresholds |
Each experiment is separately documented and reproducible.
| `ðŸ”ºVAE_model.ipynb` | VAE and Î²-VAE models with KL warm-up and visualization |
| `ðŸ”ºShap_Interpretability.ipynb` | SHAP analysis for best AE model interpretability |
| `ðŸ”ºtraditional_models_with_tuning.ipynb` | Baseline classical models: One-Class SVM, LOF, Isolation Forest, Elliptic Envelope and evaluation|
---

## ðŸ”¬ Models and Experiments

The goal is to design and evaluate anomaly detection models under a robust framework. Models evaluated include:

### âœ… AutoEncoder (AE)

Try basic AE model first, then design 6 experiments below to refine it
- Shallow & Deep AE comparison
- Bottleneck dimension sweep (4, 8, 16, 32)
- Activation comparison: ReLU, Tanh, ELU, SELU, LeakyReLU
- Loss strategies: standard MSE vs. mixed loss
- Optimizers: Adam, AdamW, SGD
- Thresholding: Percentile-based, F1-maximization

ðŸ“Š **Best AE Result**  
F1 = 0.7527 | AUC = 0.9907 (Shallow AE, tanh, bottleneck=16, AdamW)

### âœ… Variational AutoEncoder (VAE)

- Basic VAE
- KL divergence weighting (`Î²`-VAE)
- KL annealing (warm-up strategy)

âŒ Paused further optimization on the VAE series because they underperformed compared to AE on this dataset

### ðŸ†š Traditional Models

- One-Class SVM, Isolation Forest, Local Outlier Factor, Elliptic Envelope
- Trained on **normal-only samples**, validated on mixed data
- AE consistently outperformed all classical methods in F1/AUC

---
## ðŸ” SHAP Interpretability

To better understand the behavior of the trained AutoEncoder (AE) model, I employed **SHAP (SHapley Additive exPlanations)** to provide insights into which input features most influence anomaly scores.

### â“ Why SHAP?

SHAP is a unified approach to explain the output of any machine learning model. It computes the contribution of each feature by considering all possible combinations of features. In the context of anomaly detection, this allows us to:

- Identify which features contribute most to reconstruction errors
- Investigate what drives anomalous vs. normal behavior
- Build trust and transparency into the modelâ€™s decisions

### ðŸ“ˆ Summary Plot

The SHAP summary plot ranks features by average impact and shows how high/low values of each feature affect anomaly detection. Here is the summary plot for our final selected AE model:

### Workflow

- A subset of **anomalous** samples was selected for explanation
- SHAP values were computed using a background sample from normal data
- We visualized global importance using summary and bar plots

SHAP results demonstrated the AE model's ability to focus on semantically meaningful indicators of attack behavior.

> ðŸ“ For detailed SHAP plots and explanation workflow, see `notebooks/Shap_Interpretability.ipynb`.


## ðŸ”­ Future Work
- Add interpretability via SHAP / LIME
- Hyperparameter tuning and AE/VAE deepening
- Build modular `train_runner.py`
- Add CLI + logging support for reproducible runs
- **Strict â€œTrain-Onlyâ€ Feature Engineering**  
  Implement a fully pipeline-based workflow (e.g. custom `RareCategoryGrouper`, correlation filters, PCA) that is **fitted on the training set only** and then applied to validation/test sets. This will eliminate any potential leakage from unsupervised transforms.

- **Compare Leakage-Aware vs. Pragmatic Approaches**  
  Empirically evaluate the impact on AE/VAE and traditional model performance when grouping rare categories and dropping correlations using (a) full-data rules vs. (b) train-only rules.

- **Production-Ready SKLearn Pipeline**  
  Wrap the entire preprocess â†’ train â†’ evaluate steps into an `sklearn.Pipeline` (or TensorFlow Transform) to support reproducible CI/CD retraining and deployment.

- **Automated CI/CD Tests**  
  Add unit/integration tests that assert no data-leakageâ€”e.g. confirm that transformers fitted on train data yield identical outputs on unseen samples.

- **Monitoring & Drift Detection in Deployment**  
  In a deployed setting, continuously monitor feature distributions and retrain the â€œtrain-onlyâ€ pipeline when concept drift is detected.


---

## ðŸ‘©â€ðŸ’» Author

Created by [Your Name] Â· Portfolio Project  
> *Feel free to fork, study or reuse parts of this repo in your own data science learning path.*