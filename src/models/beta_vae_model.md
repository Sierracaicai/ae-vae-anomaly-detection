# ğŸ“¦ Beta-VAE Model

This module defines a custom implementation of a Î²-VAE (Beta Variational Autoencoder), which introduces a weighting factor Î² on the KL-divergence to control disentanglement in the latent space.

---

## ğŸ”§ Architecture

### ğŸ§  Encoder

- `Dense(64)` + `ReLU` â†’ `BatchNorm` â†’ `Dropout`
- `Dense(32)` + `ReLU`
- Latent:
  - `z_mean`: `Dense(encoding_dim)`
  - `z_log_var`: `Dense(encoding_dim)`
  - `z`: Sampled via `Sampling` layer using reparameterization trick

### ğŸ§ª Decoder

- `Dense(32)` + `ReLU` â†’ `BatchNorm` â†’ `Dropout`
- `Dense(64)` + `ReLU`
- `Dense(input_dim)` with linear activation for reconstruction

---

## ğŸ§® Loss Function

The total loss is defined as:

\[
\mathcal{L} = \text{Reconstruction Loss} + \beta \cdot \text{KL Divergence}
\]

- **Reconstruction Loss**: MSE
- **KL Divergence**:
\[
-0.5 \sum(1 + \log(\sigma^2) - \mu^2 - \sigma^2)
\]

---

## âš™ï¸ Key Components

### ğŸ”¹ `Sampling` Layer

Reparameterization trick:
```python
z = z_mean + tf.exp(0.5 * z_log_var) * epsilon
```

### ğŸ”¹ `BetaVAE` Class

Inherits from `tf.keras.Model`, and overrides:
- `call()`: For inference
- `train_step()` and `test_step()`: To apply custom loss with weighted KL term

Tracks:
- `total_loss`
- `reconstruction_loss`
- `kl_loss`

### ğŸ”¹ `KLAnnealing` Class

Supports warm-up strategy for Î²:
```python
beta = start_beta + (target_beta - start_beta) * (epoch / total_epochs)
```

---

## ğŸ‹ï¸ Training Utilities

### ğŸ”¸ `train_vae(...)`

- Compiles and trains the model with callbacks:
  - Early stopping
  - Model checkpoint
  - Learning rate scheduler

### ğŸ”¸ `plot_vae_history(...)`

- Visualizes loss curves:
  - Total Loss
  - Reconstruction Loss
  - KL Loss

---

## ğŸ“ Output

- Best model saved to `.h5` file
- Loss curve saved to `vae_loss_plot.png`

---

## âœ… Summary

The Î²-VAE improves control over latent variable disentanglement by weighting the KL divergence term. This implementation supports:
- Custom Î² values
- Î² warm-up scheduling
- Custom training loop for loss monitoring
- Visualization of training dynamics
